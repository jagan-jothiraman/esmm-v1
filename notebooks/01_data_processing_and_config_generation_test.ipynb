{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Data Processing and EasyRec Config Generation\n",
    "\n",
    "This notebook demonstrates how to use the scripts from the `src` directory to:\n",
    "1. Simulate splitting data (as `split_dataset.py` would do from GCS).\n",
    "2. Use a sample of this data with `auto_feature_config_generator.py` to create `input_fields` and `feature_config` sections.\n",
    "3. Use `generate_and_run_train.py` to populate the `esmm_pipeline_template.config` with these generated sections and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# Add src to path to allow direct imports if package not installed via pip install -e .\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added {module_path} to sys.path\")\n",
    "\n",
    "try:\n",
    "    from data_processing import auto_feature_config_generator\n",
    "    # from data_processing import split_dataset # Not directly calling its main for this test\n",
    "    # from training import generate_and_run_train # Will call this script via subprocess\n",
    "    print(\"Successfully imported 'auto_feature_config_generator'.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    print(\"Please ensure 'src' is in PYTHONPATH or the package is installed (e.g., pip install -e .)\")\n",
    "\n",
    "# Mock GCS paths and local paths for testing\n",
    "# These would be replaced with actual GCS paths in a real scenario.\n",
    "MOCK_GCS_RAW_DATA_INPUT_DAY_PATH = \"gs://mock-bucket/raw_data/20231026/\" # Input for split_dataset.py (not used directly here)\n",
    "MOCK_GCS_PROCESSED_PATH_PREFIX = \"gs://mock-bucket/processed_data/\"      # Output prefix for split_dataset.py\n",
    "MOCK_GCS_MODEL_OUTPUT_PATH_PREFIX = \"gs://mock-bucket/model_output/\"    # Output for training model_dir\n",
    "\n",
    "# For generate_and_run_train.py\n",
    "MOCK_GCS_TRAIN_DIR_FOR_SCHEMA_INF = os.path.join(MOCK_GCS_PROCESSED_PATH_PREFIX, \"20231026/train/\") # Used for schema inference\n",
    "MOCK_GCS_TRAIN_DATA_PATH = os.path.join(MOCK_GCS_PROCESSED_PATH_PREFIX, \"20231026/train/\")\n",
    "MOCK_GCS_EVAL_DATA_PATH = os.path.join(MOCK_GCS_PROCESSED_PATH_PREFIX, \"20231026/validation/\")\n",
    "MOCK_GCS_MODEL_DIR = os.path.join(MOCK_GCS_MODEL_OUTPUT_PATH_PREFIX, \"20231026/\")\n",
    "\n",
    "CONFIG_TEMPLATE_PATH = \"../configs/esmm_pipeline_template.config\"\n",
    "GENERATED_CONFIG_PATH = \"../configs/generated_notebook_test_pipeline.config\"\n",
    "\n",
    "print(f\"Config template path: {os.path.abspath(CONFIG_TEMPLATE_PATH)}\")\n",
    "print(f\"Generated config will be at: {os.path.abspath(GENERATED_CONFIG_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulate Data Splitting (using logic from `split_dataset.py`)\n",
    "\n",
    "The `split_dataset.py` script would typically read from a GCS path like `MOCK_GCS_RAW_DATA_INPUT_DAY_PATH`, process multiple Parquet files, perform splits, and save them to paths like `MOCK_GCS_TRAIN_DATA_PATH` and `MOCK_GCS_EVAL_DATA_PATH`.\n",
    "\n",
    "For this notebook, we'll directly generate a sample DataFrame that represents the kind of data `auto_feature_config_generator.py` would expect for schema inference. This sample would conceptually come from the *training split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame simulates a sample taken from the training data after splitting.\n",
    "# auto_feature_config_generator.load_sample_from_gcs_parquet would be used by generate_and_run_train.py\n",
    "# to (mock) load this from a GCS path.\n",
    "try:\n",
    "    sample_df_for_schema = auto_feature_config_generator.get_sample_dataframe(num_rows=500) \n",
    "    print(\"Sample DataFrame for schema inference created:\")\n",
    "    sample_df_for_schema.head()\n",
    "    # sample_df_for_schema.info() # Uncomment for more detail\n",
    "except Exception as e:\n",
    "    print(f\"Error generating sample DataFrame: {e}\")\n",
    "    sample_df_for_schema = pd.DataFrame() # Ensure it exists for next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate EasyRec Feature Configurations\n",
    "\n",
    "Using `auto_feature_config_generator.py` functions with the sample DataFrame to produce protobuf snippets for `input_fields` and `feature_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    classified_features = auto_feature_config_generator.classify_features(sample_df_for_schema)\n",
    "    print(\"Classified Features:\", classified_features)\n",
    "    \n",
    "    input_fields_proto_str = auto_feature_config_generator.generate_input_fields_proto(classified_features)\n",
    "    feature_config_proto_str = auto_feature_config_generator.generate_feature_config_proto(classified_features)\n",
    "\n",
    "    print(\"\\n--- Input Fields Proto Snippet ---\")\n",
    "    print(input_fields_proto_str)\n",
    "    print(\"\\n--- Feature Config Proto Snippet ---\")\n",
    "    print(feature_config_proto_str)\n",
    "except Exception as e:\n",
    "    print(f\"Error during feature configuration generation: {e}\")\n",
    "    classified_features = {} # Ensure it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Populate Full EasyRec Pipeline Configuration\n",
    "\n",
    "This step uses the `generate_and_run_train.py` script to take the template config and populate it with:\n",
    "1. The auto-generated `input_fields` and `feature_config` (which the script internally generates using `auto_feature_config_generator`).\n",
    "2. GCS paths for training data, evaluation data, and model output.\n",
    "3. Automated feature groupings (user, item) based on naming conventions.\n",
    "4. Other parameters like batch size, epochs (if overridden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the command to call generate_and_run_train.py\n",
    "# The script will internally call auto_feature_config_generator functions, \n",
    "# including the (mock) GCS sample loading and automated feature grouping.\n",
    "\n",
    "generate_script_path = os.path.join(module_path, \"training/generate_and_run_train.py\")\n",
    "\n",
    "cmd_args = [\n",
    "    \"python\", generate_script_path,\n",
    "    \"--gcs_train_data_for_schema_inference\", MOCK_GCS_TRAIN_DIR_FOR_SCHEMA_INF, # Path for schema sample loading\n",
    "    \"--template_config_path\", CONFIG_TEMPLATE_PATH,\n",
    "    \"--output_config_path\", GENERATED_CONFIG_PATH,\n",
    "    \"--gcs_processed_data_path_train\", MOCK_GCS_TRAIN_DATA_PATH,\n",
    "    \"--gcs_processed_data_path_eval\", MOCK_GCS_EVAL_DATA_PATH,\n",
    "    \"--gcs_model_dir_path\", MOCK_GCS_MODEL_DIR,\n",
    "    # --user_features and --item_features are no longer used by generate_and_run_train.py\n",
    "    # Feature grouping is now automated within the script.\n",
    "    # \"--execute_training\", # Uncomment to try to execute training (requires EasyRec env)\n",
    "]\n",
    "\n",
    "print(f\"Ensuring script path exists: {generate_script_path}\")\n",
    "if not os.path.exists(generate_script_path):\n",
    "    print(f\"Error: generate_and_run_train.py not found at {generate_script_path}\")\n",
    "else:\n",
    "    print(f\"Running command: {' '.join(cmd_args)}\\n\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd_args, capture_output=True, text=True, check=False) # check=False to see output even on error\n",
    "        \n",
    "        print(\"--- STDOUT from generate_and_run_train.py ---\")\n",
    "        print(result.stdout)\n",
    "        print(\"--- STDERR from generate_and_run_train.py ---\")\n",
    "        print(result.stderr)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(\"\\nConfig generation script ran successfully.\")\n",
    "            print(\"Generated config content (from file):\")\n",
    "            try:\n",
    "                with open(GENERATED_CONFIG_PATH, 'r') as f:\n",
    "                    print(f.read())\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: Generated config file not found at {GENERATED_CONFIG_PATH}\")\n",
    "        else:\n",
    "            print(f\"\\nError running config generation script (return code: {result.returncode}).\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 'python' executable not found. Please ensure Python is in your PATH.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "        \n",
    "- Review the `configs/generated_notebook_test_pipeline.config` to see the populated values.\n",
    "- Replace mock GCS paths in the notebook and in actual script calls with real paths when ready.\n",
    "- Ensure the GCS sample reading in `auto_feature_config_generator.load_sample_from_gcs_parquet` is implemented to use real GCS data if you need true schema inference from your dataset.\n",
    "- To run EasyRec training using the generated config (ensure EasyRec is installed and environment is active):\n",
    "  ```bash\n",
    "  python -m easy_rec.python.train_eval --pipeline_config_path configs/generated_notebook_test_pipeline.config\n",
    "  ```\n",
    "- To evaluate model predictions (after generating them), use `src/evaluation/evaluate_model.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
